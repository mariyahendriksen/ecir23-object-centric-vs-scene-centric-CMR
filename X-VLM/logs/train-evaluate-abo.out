Working with ABO!
Start training!
/home/mbiriuk/anaconda3/envs/xvlm/lib/python3.7/site-packages/torch/cuda/__init__.py:104: UserWarning: 
NVIDIA RTX A6000 with CUDA capability sm_86 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75.
If you want to use the NVIDIA RTX A6000 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/

  warnings.warn(incompatible_device_warn.format(device_name, capability, " ".join(arch_list), device_name))
| distributed init (rank 0): env://
Creating model
### Loading pretrained vision encoder
Position interpolate vision_encoder.layers.0.blocks.0.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.0.blocks.1.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.1.blocks.0.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.1.blocks.1.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.0.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.1.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.2.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.3.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.4.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.5.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.6.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.7.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.8.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.9.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.10.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.11.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.12.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.13.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.14.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.15.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.16.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.2.blocks.17.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.3.blocks.0.attn.relative_position_bias_table from 13x13 to 23x23
Position interpolate vision_encoder.layers.3.blocks.1.attn.relative_position_bias_table from 13x13 to 23x23
### Loading pretrained text encoder
load checkpoint from /ivi/ilps/personal/mbiriuk/repro/X-VLM/4m_base_model_state_step_199999.th
missing_keys:  []
unexpected_keys:  ['bbox_head.0.weight', 'bbox_head.0.bias', 'bbox_head.1.weight', 'bbox_head.1.bias', 'bbox_head.3.weight', 'bbox_head.3.bias', 'text_encoder.cls.predictions.bias', 'text_encoder.cls.predictions.transform.dense.weight', 'text_encoder.cls.predictions.transform.dense.bias', 'text_encoder.cls.predictions.transform.LayerNorm.weight', 'text_encoder.cls.predictions.transform.LayerNorm.bias', 'text_encoder.cls.predictions.decoder.weight', 'text_encoder.cls.predictions.decoder.bias']
### Total Params:  213959547
Traceback (most recent call last):
  File "Retrieval.py", line 382, in <module>
    main(args, config)
  File "Retrieval.py", line 236, in main
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])
  File "/home/mbiriuk/anaconda3/envs/xvlm/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 410, in __init__
    self._sync_params_and_buffers(authoritative_rank=0)
  File "/home/mbiriuk/anaconda3/envs/xvlm/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 420, in _sync_params_and_buffers
    authoritative_rank)
  File "/home/mbiriuk/anaconda3/envs/xvlm/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 979, in _distributed_broadcast_coalesced
    self.process_group, tensors, buffer_size, authoritative_rank
RuntimeError: CUDA error: no kernel image is available for execution on the device
Traceback (most recent call last):
  File "/home/mbiriuk/anaconda3/envs/xvlm/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/mbiriuk/anaconda3/envs/xvlm/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/mbiriuk/anaconda3/envs/xvlm/lib/python3.7/site-packages/torch/distributed/launch.py", line 260, in <module>
    main()
  File "/home/mbiriuk/anaconda3/envs/xvlm/lib/python3.7/site-packages/torch/distributed/launch.py", line 256, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/home/mbiriuk/anaconda3/envs/xvlm/bin/python3', '-u', 'Retrieval.py', '--config', 'configs/retrieval_abo.yaml', '--output_dir', 'output/itr_abo', '--bs', '-1', '--checkpoint', '/ivi/ilps/personal/mbiriuk/repro/X-VLM/4m_base_model_state_step_199999.th']' returned non-zero exit status 1.
NNODES,  1
NPROC_PER_NODE,  8
MASTER_ADDR,  SET_IT
MASTER_PORT,  12345
NODE_RANK,  0
### warning: the settings for distributed training is not filled (ignore this if you only use one node)
### warning: you have not set the path to hadoop_bin (ignore this if you don't use HDFS)
Done with training!
End of script!
