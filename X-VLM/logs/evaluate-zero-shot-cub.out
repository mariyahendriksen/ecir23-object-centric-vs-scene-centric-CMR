| distributed init (rank 0): env://
Creating model
Traceback (most recent call last):
  File "Retrieval.py", line 398, in <module>
    main(args, config)
  File "Retrieval.py", line 246, in main
    model.load_pretrained(args.checkpoint, config, is_eval=args.evaluate)
  File "/home/mbiriuk/X-VLM/models/model_retrieval.py", line 15, in load_pretrained
    msg = self.load_state_dict(state_dict, strict=False)
  File "/home/mbiriuk/anaconda3/envs/xvlm2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1052, in load_state_dict
    self.__class__.__name__, "\n\t".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for XVLM:
	size mismatch for vision_encoder.layers.0.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 4]) from checkpoint, the shape in current model is torch.Size([529, 4]).
	size mismatch for vision_encoder.layers.0.blocks.0.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.0.blocks.1.attn_mask: copying a param with shape torch.Size([64, 49, 49]) from checkpoint, the shape in current model is torch.Size([64, 144, 144]).
	size mismatch for vision_encoder.layers.0.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 4]) from checkpoint, the shape in current model is torch.Size([529, 4]).
	size mismatch for vision_encoder.layers.0.blocks.1.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.1.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 8]) from checkpoint, the shape in current model is torch.Size([529, 8]).
	size mismatch for vision_encoder.layers.1.blocks.0.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.1.blocks.1.attn_mask: copying a param with shape torch.Size([16, 49, 49]) from checkpoint, the shape in current model is torch.Size([16, 144, 144]).
	size mismatch for vision_encoder.layers.1.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 8]) from checkpoint, the shape in current model is torch.Size([529, 8]).
	size mismatch for vision_encoder.layers.1.blocks.1.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.0.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.1.attn_mask: copying a param with shape torch.Size([4, 49, 49]) from checkpoint, the shape in current model is torch.Size([4, 144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.1.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.2.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.2.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.3.attn_mask: copying a param with shape torch.Size([4, 49, 49]) from checkpoint, the shape in current model is torch.Size([4, 144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.3.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.3.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.4.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.4.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.5.attn_mask: copying a param with shape torch.Size([4, 49, 49]) from checkpoint, the shape in current model is torch.Size([4, 144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.5.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.5.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.6.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.6.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.7.attn_mask: copying a param with shape torch.Size([4, 49, 49]) from checkpoint, the shape in current model is torch.Size([4, 144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.7.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.7.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.8.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.8.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.9.attn_mask: copying a param with shape torch.Size([4, 49, 49]) from checkpoint, the shape in current model is torch.Size([4, 144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.9.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.9.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.10.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.10.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.11.attn_mask: copying a param with shape torch.Size([4, 49, 49]) from checkpoint, the shape in current model is torch.Size([4, 144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.11.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.11.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.12.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.12.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.13.attn_mask: copying a param with shape torch.Size([4, 49, 49]) from checkpoint, the shape in current model is torch.Size([4, 144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.13.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.13.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.14.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.14.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.15.attn_mask: copying a param with shape torch.Size([4, 49, 49]) from checkpoint, the shape in current model is torch.Size([4, 144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.15.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.15.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.16.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.16.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.17.attn_mask: copying a param with shape torch.Size([4, 49, 49]) from checkpoint, the shape in current model is torch.Size([4, 144, 144]).
	size mismatch for vision_encoder.layers.2.blocks.17.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 16]) from checkpoint, the shape in current model is torch.Size([529, 16]).
	size mismatch for vision_encoder.layers.2.blocks.17.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.3.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 32]) from checkpoint, the shape in current model is torch.Size([529, 32]).
	size mismatch for vision_encoder.layers.3.blocks.0.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
	size mismatch for vision_encoder.layers.3.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([169, 32]) from checkpoint, the shape in current model is torch.Size([529, 32]).
	size mismatch for vision_encoder.layers.3.blocks.1.attn.relative_position_index: copying a param with shape torch.Size([49, 49]) from checkpoint, the shape in current model is torch.Size([144, 144]).
Traceback (most recent call last):
  File "/home/mbiriuk/anaconda3/envs/xvlm2/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/mbiriuk/anaconda3/envs/xvlm2/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/mbiriuk/anaconda3/envs/xvlm2/lib/python3.7/site-packages/torch/distributed/launch.py", line 260, in <module>
    main()
  File "/home/mbiriuk/anaconda3/envs/xvlm2/lib/python3.7/site-packages/torch/distributed/launch.py", line 256, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/home/mbiriuk/anaconda3/envs/xvlm2/bin/python3', '-u', 'Retrieval.py', '--config', 'configs/retrieval_cub.yaml', '--output_dir', 'output/itr_zero_shot_cub', '--bs', '-1', '--checkpoint', '/ivi/ilps/personal/mbiriuk/repro/X-VLM/4m_base_model_state_step_199999.th', '--evaluate']' returned non-zero exit status 1.
NNODES,  1
NPROC_PER_NODE,  8
MASTER_ADDR,  SET_IT
MASTER_PORT,  12345
NODE_RANK,  0
### warning: the settings for distributed training is not filled (ignore this if you only use one node)
### warning: you have not set the path to hadoop_bin (ignore this if you don't use HDFS)
